{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "%reload_ext dotenv\n",
                "%dotenv"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import time\n",
                "import json\n",
                "import random\n",
                "import warnings\n",
                "import string\n",
                "from pathlib import Path\n",
                "from itertools import takewhile, repeat\n",
                "from math import ceil\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from tqdm.auto import tqdm\n",
                "from sqlalchemy import create_engine, types\n",
                "\n",
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Operational Data Generation\n",
                "\n",
                "The purpose of [Yelp Dataset](https://www.yelp.com/dataset) is to be used for research. Because of that some information present in the entities of the data\n",
                "are designed for analytics such as aggregated data (e.g. number of stars) or some transformations. The idea of this notebook is to use this data and transforms in a way as close as possible from what appears to be a real-world operational data scenario (with the minimum required number of transformations). This data will be further used in other data mesh steps. Some important assumptions/takeways of this scenario. For more details about the dataset see [Yelp Dataset Documentation](https://www.yelp.com/dataset/documentation/main).\n",
                "\n",
                "**Running this script:** Create a file .env with the environment variables described in the secrets section. They will be loaded automatically when running this notebook"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "POSTGRESQL_USER = os.environ.get(\"POSTGRESQL_USER\")\n",
                "POSTGRESQL_PASSWORD = os.environ.get(\"POSTGRESQL_PASSWORD\")\n",
                "POSTGRESQL_HOST = os.environ.get(\"POSTGRESQL_HOST\")\n",
                "POSTGRESQL_PORT = os.environ.get(\"POSTGRESQL_PORT\")\n",
                "\n",
                "DATA_FOLDER = Path('../data')\n",
                "CHUNK_SIZE = 1000 # Number of lines to be processed at a time\n",
                "\n",
                "BUSINESS_DATASET_PATH = Path(DATA_FOLDER/'yelp_academic_dataset_business.json')\n",
                "USERS_DATASET_PATH = Path(DATA_FOLDER/'yelp_academic_dataset_user.json')\n",
                "CHECKIN_DATASET_PATH = Path(DATA_FOLDER/'yelp_academic_dataset_checkin.json')\n",
                "REVIEWS_DATASET_PATH = Path(DATA_FOLDER/'yelp_academic_dataset_review.json')\n",
                "TIP_DATASET_PATH = Path(DATA_FOLDER/'yelp_academic_dataset_tip.json')\n",
                "\n",
                "BUSINESS_DATABASE_NAME = \"domain_business\"\n",
                "USERS_DATABASE_NAME = \"domain_user\"\n",
                "CHECKIN_DATABASE_NAME = \"domain_checkin\"\n",
                "EVALUATIONS_DATABASE_NAME = \"domain_evaluations\"\n",
                "\n",
                "# Set to maintain reproducibility on check-in domain\n",
                "random.seed(42) "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Support Functions**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {
                "editable": true,
                "slideshow": {
                    "slide_type": ""
                },
                "tags": []
            },
            "outputs": [],
            "source": [
                "def rawincount(path: Path) -> int:\n",
                "    \"\"\"Count the number of lines of a file without loading all into memory\n",
                "    Args:\n",
                "        filename: Path of the file to count the lines.\n",
                "    Return:\n",
                "        int: number of lines on the file.\n",
                "    Ref:\n",
                "        https://stackoverflow.com/questions/845058/how-to-get-line-count-of-a-large-file-cheaply-in-python\n",
                "    \"\"\"\n",
                "    f = open(path, 'rb')\n",
                "    bufgen = takewhile(\n",
                "        lambda x: x, (f.raw.read(1024*1024) for _ in repeat(None))\n",
                "    )\n",
                "    return sum(buf.count(b'\\n') for buf in bufgen)\n",
                "\n",
                "def random_id_generator(length: int = 22) -> str:\n",
                "    characters = string.ascii_letters + string.digits\n",
                "    return ''.join(random.choices(characters, k=length))\n",
                "    "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Business Domain\n",
                "\n",
                "The business domain contain information about each business in Yelp such as address, location, categories and other variables. To populate this domain we only need to drop some columns that we assume that do not exists as operational information since they are aggregations from other domains (i.e. `stars` and `reviews_count` from the **evaluations** domain)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with open(BUSINESS_DATASET_PATH, mode=\"r\") as f:\n",
                "    print(f\"Example: {f.readline()}\")\n",
                "\n",
                "business_engine = create_engine(\n",
                "    f\"postgresql://{POSTGRESQL_USER}:{POSTGRESQL_PASSWORD}@{POSTGRESQL_HOST}:{POSTGRESQL_PORT}/{BUSINESS_DATABASE_NAME}\"\n",
                ")\n",
                "\n",
                "n_lines = rawincount(BUSINESS_DATASET_PATH)\n",
                "with business_engine.connect() as connection:\n",
                "    for chunk in tqdm(\n",
                "        pd.read_json(BUSINESS_DATASET_PATH, chunksize=CHUNK_SIZE, lines=True),\n",
                "        total=ceil(n_lines / CHUNK_SIZE),\n",
                "    ):\n",
                "        chunk = chunk.rename(columns={\"business_id\": \"id\"})\n",
                "        chunk = chunk.drop(columns=[\"stars\", \"review_count\"])\n",
                "        chunk.to_sql(\n",
                "            \"business\",\n",
                "            con=connection,\n",
                "            if_exists=\"append\",\n",
                "            index=False,\n",
                "            method=\"multi\",\n",
                "            dtype={\n",
                "                \"id\": \"TEXT PRIMARY KEY\",\n",
                "                \"attributes\": types.JSON,\n",
                "                \"hours\": types.JSON,\n",
                "                \"is_open\": types.BOOLEAN,\n",
                "            },\n",
                "        )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Users Domain\n",
                "\n",
                "The users domain contain basic information about the users such as names, their friends, when the user joined Yelp and many other information. Populate this domain requires populating 3 tables: `users`, `elite_members` and `friends`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "users_engine = create_engine(f'postgresql://{POSTGRESQL_USER}:{POSTGRESQL_PASSWORD}@{POSTGRESQL_HOST}:{POSTGRESQL_PORT}/{USERS_DATABASE_NAME}')\n",
                "\n",
                "with open(USERS_DATASET_PATH, mode='r') as f:\n",
                "    example = json.loads(f.readline())\n",
                "    del example[\"friends\"] # Removed only for a more readable output\n",
                "    print(f\"Example: {example}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Tables: users and elite_members \n",
                "\n",
                "On the first we are going to populate 2 tables, the `users` that contain basic information about an user and the `elite_members` table that contain all the elite members by the year. Here we follow the same approach for the business dataset by removing some data from other domains (e.g. `review_count`, `funny` and many others)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_elite_member_table(df: pd.DataFrame) -> pd.DataFrame:\n",
                "    \"\"\"Transform the `elite` variable into a table\n",
                "\n",
                "    This function gets the `elite` string variable (years as elite\n",
                "    with a ',' separator and transforms into a Datataframe with\n",
                "    'user_id' and 'year' as columns of the data.\n",
                "\n",
                "    Args:\n",
                "        df: Dataframe that contains the elite members variable\n",
                "\n",
                "    Returns:\n",
                "        pd.Dataframe: Dataframe with 'user_id' and 'year' as columns\n",
                "    \"\"\"\n",
                "    return_data = []\n",
                "    for i, row in df.iterrows():\n",
                "        for year in row[\"elite\"].split(\",\"):\n",
                "            if (\n",
                "                year == \"20\"\n",
                "            ):  # Just a small correction where all 2020 year appears as 20,20\n",
                "                return_data.append({\"user_id\": row[\"id\"], \"year\": 2020})\n",
                "            elif year != \"\":\n",
                "                return_data.append({\"user_id\": row[\"id\"], \"year\": int(year)})\n",
                "    return pd.DataFrame(return_data).drop_duplicates()\n",
                "\n",
                "\n",
                "n_lines = rawincount(USERS_DATASET_PATH)\n",
                "with users_engine.connect() as connection:\n",
                "    for chunk in tqdm(\n",
                "        pd.read_json(USERS_DATASET_PATH, chunksize=CHUNK_SIZE, lines=True),\n",
                "        total=ceil(n_lines / CHUNK_SIZE),\n",
                "    ):\n",
                "        chunk = chunk.rename(columns={\"user_id\": \"id\"})\n",
                "\n",
                "        # Users table\n",
                "        users_chunk = chunk[[\"id\", \"name\", \"yelping_since\"]]\n",
                "        users_chunk.to_sql(\n",
                "            \"users\",\n",
                "            con=connection,\n",
                "            if_exists=\"append\",\n",
                "            index=False,\n",
                "            method=\"multi\",\n",
                "        )\n",
                "\n",
                "        # Elite Table\n",
                "        elite_chunk = generate_elite_member_table(chunk)\n",
                "        elite_chunk.to_sql(\n",
                "            \"elite_members\",\n",
                "            con=connection,\n",
                "            if_exists=\"append\",\n",
                "            index=False,\n",
                "            method=\"multi\",\n",
                "        )\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Tables: friends\n",
                "\n",
                "The `friends` table will represent the connection between user as a bi-directional graph using the user ids as foreign keys (and nodes). Also, some users in the friends list do not exists in the `users` database. To solve this problem the approach was to get only the users that exists and insert as friends."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'user_id': 'qVc8ODYU5SZjKXVBgXdI7w', 'name': 'Walker', 'review_count': 585, 'yelping_since': '2007-01-25 16:47:26', 'useful': 7217, 'funny': 1259, 'cool': 5994, 'elite': '2007', 'fans': 267, 'average_stars': 3.91, 'compliment_hot': 250, 'compliment_more': 65, 'compliment_profile': 55, 'compliment_cute': 56, 'compliment_list': 18, 'compliment_note': 232, 'compliment_plain': 844, 'compliment_cool': 467, 'compliment_funny': 467, 'compliment_writer': 239, 'compliment_photos': 180}\n"
                    ]
                }
            ],
            "source": [
                "with open(USERS_DATASET_PATH, mode=\"r\") as f:\n",
                "    users_list = set([json.loads(line)[\"user_id\"] for line in f])\n",
                "\n",
                "whn_lines = rawincount(USERS_DATASET_PATH)\n",
                "with users_engine.connect() as connection:\n",
                "    for chunk in tqdm(\n",
                "        pd.read_json(USERS_DATASET_PATH, chunksize=CHUNK_SIZE, lines=True),\n",
                "        total=ceil(n_lines / CHUNK_SIZE),\n",
                "        desc=\"Chunks\",\n",
                "    ):\n",
                "        chunk = chunk[[\"user_id\", \"friends\"]]\n",
                "        chunk.friends = chunk.friends.str.split(\", \")\n",
                "        chunk.friends = chunk.friends.transform(\n",
                "            lambda x: set(x).intersection(users_list)\n",
                "        )\n",
                "        chunk = chunk.explode(\"friends\")\n",
                "        chunk = chunk.rename(\n",
                "            columns={\"user_id\": \"previous_user\", \"friends\": \"next_user\"}\n",
                "        )\n",
                "        chunk = chunk.dropna()\n",
                "        chunk.to_sql(\n",
                "            \"friends\",\n",
                "            con=connection,\n",
                "            if_exists=\"append\",\n",
                "            index=False,\n",
                "            method=\"multi\",\n",
                "        )\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Checkin Domain\n",
                "\n",
                "This domain contains information about each check-in made by an user to a business. To populate this database some previous information is required: As can be seen in the example below the dataset made available by Yelp does not contain the information about the user that did the checkin into the business, this is expected since this is sensitive and could have been used to identify an user. Since we are simulating a real world dataset, this information is important and our approach will be to randomly select (with repetition) an existing `user_id` for each business checkin. This is not an optimal way to simulate but should be sufficient to create products and dashboards based on this information.\n",
                "\n",
                "<div class=\"alert alert-block alert-info\"> <b>Note:</b> To maintain reproducibility note that the random seed is fixed on the beginning of this notebook</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "checkin_engine = create_engine(\n",
                "    f\"postgresql://{POSTGRESQL_USER}:{POSTGRESQL_PASSWORD}@{POSTGRESQL_HOST}:{POSTGRESQL_PORT}/{CHECKIN_DATABASE_NAME}\"\n",
                ")\n",
                "\n",
                "with open(CHECKIN_DATASET_PATH, mode=\"r\") as f:\n",
                "    print(f\"Example: {f.readline()}\")\n",
                "\n",
                "with open(USERS_DATASET_PATH, mode=\"r\") as f:\n",
                "    users_list = set([json.loads(line)[\"user_id\"] for line in f])\n",
                "\n",
                "n_lines = rawincount(CHECKIN_DATASET_PATH)\n",
                "with checkin_engine.connect() as connection:\n",
                "    for chunk in tqdm(\n",
                "        pd.read_json(CHECKIN_DATASET_PATH, chunksize=CHUNK_SIZE, lines=True),\n",
                "        total=ceil(n_lines / CHUNK_SIZE),\n",
                "        desc=\"Chunks\",\n",
                "    ):\n",
                "        chunk.date = chunk.date.str.split(\", \")\n",
                "        chunk = chunk.explode(\"date\")\n",
                "        chunk[\"user_id\"] = np.random.choice(\n",
                "            users_list, size=len(chunk), replace=True\n",
                "        )\n",
                "        chunk[\"id\"] = [random_id_generator() for _ in range(CHUNK_SIZE)] \n",
                "        chunk = chunk.rename(columns={\"date\": \"checkin_date\"})\n",
                "        chunk.to_sql(\n",
                "            \"checkins\",\n",
                "            con=connection,\n",
                "            if_exists=\"append\",\n",
                "            index=False,\n",
                "            method=\"multi\",\n",
                "        )\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluations Domain\n",
                "\n",
                "This evaluations domain contain information about tips and reviews made by users on business. See [Yelp Dataset Documentation](https://www.yelp.com/dataset/documentation/main)) for more complete information about the variables."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {
                "editable": true,
                "slideshow": {
                    "slide_type": ""
                },
                "tags": []
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{\"review_id\":\"KU_O5udG6zpxOg-VcAEodg\",\"user_id\":\"mh_-eMZ6K5RLWhZyISBhwA\",\"business_id\":\"XQfwVwDr-v0ZS3_CbbE5Xw\",\"stars\":3.0,\"useful\":0,\"funny\":0,\"cool\":0,\"text\":\"If you decide to eat here, just be aware it is going to take about 2 hours from beginning to end. We have tried it multiple times, because I want to like it! I have been to it's other locations in NJ and never had a bad experience. \\n\\nThe food is good, but it takes a very long time to come out. The waitstaff is very young, but usually pleasant. We have just had too many experiences where we spent way too long waiting. We usually opt for another diner or restaurant on the weekends, in order to be done quicker.\",\"date\":\"2018-07-07 22:09:11\"}\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "evaluations_engine = create_engine(f'postgresql://{POSTGRESQL_USER}:{POSTGRESQL_PASSWORD}@{POSTGRESQL_HOST}:{POSTGRESQL_PORT}/{EVALUATIONS_DATABASE_NAME}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Tables: reviews"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with open(REVIEWS_DATASET_PATH, mode=\"r\") as f:\n",
                "    print(f\"Example: {f.readline()}\")\n",
                "\n",
                "n_lines = rawincount(REVIEWS_DATASET_PATH)\n",
                "with evaluations_engine.connect() as connection:\n",
                "    for chunk in tqdm(\n",
                "        pd.read_json(REVIEWS_DATASET_PATH, chunksize=CHUNK_SIZE, lines=True),\n",
                "        total=ceil(n_lines / CHUNK_SIZE),\n",
                "        desc=\"Chunks\",\n",
                "    ):\n",
                "        chunk = chunk.rename(\n",
                "            columns={\n",
                "                \"review_id\": \"id\",\n",
                "                \"useful\": \"useful_count\",\n",
                "                \"funny\": \"funny_count\",\n",
                "                \"cool\": \"cool_count\",\n",
                "                \"text\": \"content\",\n",
                "                \"date\": \"review_date\",\n",
                "            }\n",
                "        )\n",
                "        chunk.to_sql(\n",
                "            \"reviews\",\n",
                "            con=connection,\n",
                "            if_exists=\"append\",\n",
                "            index=False,\n",
                "            method=\"multi\",\n",
                "        )\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Tables: tips"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with open(TIP_DATASET_PATH, mode=\"r\") as f:\n",
                "    print(f\"Example: {f.readline()}\")\n",
                "\n",
                "n_lines = rawincount(TIP_DATASET_PATH)\n",
                "with evaluations_engine.connect() as connection:\n",
                "    for chunk in tqdm(\n",
                "        pd.read_json(TIP_DATASET_PATH, chunksize=CHUNK_SIZE, lines=True),\n",
                "        total=ceil(n_lines / CHUNK_SIZE),\n",
                "        desc=\"Chunks\",\n",
                "    ):\n",
                "        chunk = chunk.rename(columns={\"text\": \"content\", \"date\": \"tips_date\"})\n",
                "        chunk[\"id\"] = [random_id_generator() for _ in range(CHUNK_SIZE)] \n",
                "        chunk.to_sql(\n",
                "            \"tips\",\n",
                "            con=connection,\n",
                "            if_exists=\"append\",\n",
                "            index=False,\n",
                "            method=\"multi\",\n",
                "        )\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
